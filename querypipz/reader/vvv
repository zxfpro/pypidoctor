








import os
from datetime import datetime

template = """---
topic: {topic}
describe: {describe}
creation date: {date}
type: 案例
tags: []
status: false
链接: None
---
"""

def save_to_file(markdown, base_path,filename="output.md"):
    os.makedirs(base_path, exist_ok=True)
    file_path = os.path.join(base_path,filename)
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(markdown)
    print(f"Markdown content saved to {filename}")


def create_file_structure(data, base_path=".", overwrite=False):
    """
    递归创建文件夹和文件结构，增加防止覆盖和异常处理
    """
    # 检查 data 是否为字典
    if not isinstance(data, dict):
        print(f"数据类型不正确，跳过处理: {data}")
        return

    for key, value in data.items():
        if value.get("type") == "folder":
            try:
                # 创建文件夹
                folder_path = os.path.join(base_path, key)
                os.makedirs(folder_path, exist_ok=True)
                
                # 创建与文件夹同名的 Markdown 文件
                folder_md_file = os.path.join(folder_path, f"{key}.md")
                
                # 检查文件是否已存在
                if os.path.exists(folder_md_file) and not overwrite:
                    print(f"文件已存在，跳过创建: {folder_md_file}")
                    continue
                
                with open(folder_md_file, "w", encoding="utf-8") as f:
                    f.write(template.format(topic=key,
                                            describe='描述',
                                            date=datetime.today().strftime("%Y-%m-%d"),
                                            ))
                    f.write(value.get("text", ""))
                
                # 递归处理文件夹内容
                if "context" in value:
                    # 确保 context 是一个字典
                    if isinstance(value["context"], dict):
                        create_file_structure(value["context"], folder_path, overwrite)
                    else:
                        print(f"context 不是字典，跳过处理: {value['context']}")
            except Exception as e:
                print(f"创建文件夹或文件时出错: {e}")
        elif value.get("type") == "md":
            try:
                # 创建 Markdown 文件
                file_name = key.replace(" ", "_").lower() + ".md"
                file_path = os.path.join(base_path, file_name)
                
                # 检查文件是否已存在
                if os.path.exists(file_path) and not overwrite:
                    print(f"文件已存在，跳过创建: {file_path}")
                    continue
                
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(value.get("text", ""))
            except Exception as e:
                print(f"创建文件时出错: {e}")


def parse_markdown_to_custom_structure(markdown_content: str, folder_name: str = "Alias") -> dict:
    # 将Markdown内容按二级标题分割
    parts = markdown_content.split("\n## ")
    
    result = {}
    
    # 处理第一个部分（可能没有二级标题）
    if parts[0].strip():
        # 将第一部分作为文件夹同名文件的数据
        result[folder_name] = {
            "type": "folder",
            "context": {}  # 初始化为空字典
        }
    
    md_titles = []  # 用于存储所有md单元的标题
    
    # 处理剩下的部分
    for part in parts[1:]:
        # 将每个部分按第一个换行符分割为标题和内容
        if "\n" in part:
            title, content = part.split("\n", 1)
            title = title.replace('`','')
            md_entry = {
                "type": "md",
                "text": content.strip()
            }
            # 将MD条目直接添加到context字典中
            result[folder_name]["context"][title] = md_entry
            md_titles.append(title)  # 添加标题到md_titles列表
    
    # 动态生成folder的text字段
    if folder_name in result:
        text_content = "\n".join(f"[[{title}]]" for title in md_titles)
        result[folder_name]["text"] = text_content
    
    return result


"""

issue_document = read_github_issue(owner="tqdm",repo="tqdm")

issue_document[2]

print(issue_document[2].dict().get('text'))

"https://api.github.com/repos/tqdm/tqdm/issues/454/comments"
"""

import requests
import json
import re

def extract_comments(url):
    # 发送 GET 请求获取数据
    response = requests.get(url)
    if response.status_code != 200:
        print(f"Failed to fetch data: {response.status_code}")
        return None
    
    # 解析 JSON 数据
    try:
        data = response.json()
    except json.JSONDecodeError:
        print("Failed to parse JSON")
        return None
    
    return data

def convert_to_markdown(comments):
    markdown = ""
    
    for comment in comments:
        user = comment.get("user", {}).get("login", "Unknown User")
        created_at = comment.get("created_at", "")
        body = comment.get("body", "")
        
        # 转义 Markdown 特殊字符
        body = re.sub(r'([\\`*_{}[\]()#+\-.!])', r'\\\1', body)
        
        markdown += f"### Comment by {user} ({created_at})\n"
        markdown += f">{body}\n\n"
    
    return markdown


def get_comments(url):
    comments = extract_comments(url)
    if comments:
        markdown = convert_to_markdown(comments)
        return markdown
    return None






import os
from llama_index.readers.github import GithubRepositoryReader, GithubClient

from llama_index.readers.github import (
    GitHubRepositoryIssuesReader,
    GitHubIssuesClient,
)


def read_github_repo(owner:str="pydantic",
                     repo:str="pydantic",
                     branch:str="main",
                     filter_directories:list=None,
                     filter_file_extensions:list = None
                     ):
    """
    读取github仓库中的文档或者其他文件内容   
    owner: str 作者   
    repo: str 仓库名   
    branch : 分支   
    filter_directories: 按照文件夹筛选(包含)  ["docs/concepts"]   
    filter_file_extensions: 按照文件类型筛选 (排除)   

    """
    github_client = GithubClient(github_token=os.environ["GITHUB_TOKEN"], verbose=False)

    reader = GithubRepositoryReader(
        github_client=github_client,
        owner=owner,
        repo=repo,
        use_parser=False,
        verbose=False,
        filter_directories=(
            filter_directories or ["docs/concepts"],
            GithubRepositoryReader.FilterType.INCLUDE,
        ),
        filter_file_extensions=(
            filter_file_extensions or [
                ".png",
                ".jpg",
                ".jpeg",
                ".gif",
                ".svg",
                ".ico",
                "json",
                ".ipynb",
            ],
            GithubRepositoryReader.FilterType.EXCLUDE,
        ),
    )

    documents = reader.load_data(branch=branch)
    return documents


def read_github_issue(owner:str="pydantic",
                      repo:str="pydantic",
                      state:GitHubRepositoryIssuesReader.IssueState=None,
                      labelFilter:list = None)->'documents':
    """
    读取github仓库中的 issue 问题集   
    owner: str 作者
    repo: str 仓库名
    state : 仓库状态 state or GitHubRepositoryIssuesReader.IssueState.CLOSED,# GitHubRepositoryIssuesReader.IssueState.OPEN or .CLOSED or .ALL
    labelFilters: 按照标签筛选内容 暂未开放
    """
    github_client = GitHubIssuesClient(github_token=os.environ["GITHUB_TOKEN"], verbose=True)

    reader = GitHubRepositoryIssuesReader(
        github_client=github_client,
        owner=owner,
        repo=repo,
        verbose=True,
    )


    documents = reader.load_data(
        state=state or GitHubRepositoryIssuesReader.IssueState.CLOSED,# GitHubRepositoryIssuesReader.IssueState.OPEN or .CLOSED or .ALL
        #labelFilters= labelFilters or [("bug V2", GitHubRepositoryIssuesReader.FilterType.INCLUDE)], # 根据标签过滤
    )
    return documents


################







from llama_index.core import load_index_from_storage, StorageContext
from llama_index.core import SimpleDirectoryReader
from llama_index.core import VectorStoreIndex
from llama_index.core.readers.base import BaseReader
from llama_index.core import Document
from typing import List, Dict,Optional
import yaml
import shutil
import os



async def load(persist_dir:str,similarity_top_k=3):

    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
    index = load_index_from_storage(storage_context)
    
    return index.as_query_engine(similarity_top_k = similarity_top_k)    

async def build(file_path:str,persist_dir="./obsidian_kb"):

    file_extractor = {
        ".md": CustObsidianReader()  # 使用自定义Reader
    }
    reader = SimpleDirectoryReader(
        input_dir=file_path,
        file_extractor=file_extractor,
        recursive=True,
    )
    documents = reader.load_data()
    documents = documents[:10]
    index = VectorStoreIndex.from_documents(documents)
    if os.path.exists(persist_dir):
        shutil.rmtree(persist_dir)
    index.storage_context.persist(persist_dir=persist_dir)

    return len(documents)




################



from pinecone import Pinecone
from pinecone import ServerlessSpec
from pinecone.data.index import Index
from llama_index.core import StorageContext
from llama_index.vector_stores.pinecone import PineconeVectorStore
from llama_index.vector_stores.postgres import PGVectorStore
import os
from enum import Enum


class Pineconer():
    def __init__(self,api_key = None):
        self.api_key = api_key or os.getenv("PINECONE_API_KEY")
        self.pc = Pinecone(api_key=self.api_key)

    def create(self):
        self.pc.create_index(
            "api-documents-index",
            dimension=1536, # 维度1536
            metric="euclidean",#欧式空间 "cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )
    
    def load(self)->Index:
        pinecone_index = self.pc.Index("api-documents-index")
        return pinecone_index
    
    def get_storage(self,pinecone_index:Index)->StorageContext:
        vector_store = PineconeVectorStore(pinecone_index=pinecone_index,namespace = "test1")
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        return storage_context


class PGInfo(Enum):
    host = ''

class PortGreser():
    def __init__(self,info:PGInfo = None):
        self.info = info

    def create(self):
        pass

    
    def load(self):
        pass
    
    def get_storage(self,index_name:str)->StorageContext:
        vector_store = PGVectorStore.from_params(
            database="vector_db",
            host=self.info.host,
            password=self.info.password,
            port=self.info.port,
            user=self.info.user,
            table_name=index_name,
            embed_dim=1536,  # openai embedding dimension
            hnsw_kwargs={
                "hnsw_m": 16,
                "hnsw_ef_construction": 64,
                "hnsw_ef_search": 40,
                "hnsw_dist_method": "vector_cosine_ops",
                },
        )
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        return storage_context




from ..obragtools.reader.reader import read_github_issue,read_github_repo
from .utils import save_to_file,create_file_structure, parse_markdown_to_custom_structure, get_comments
from datetime import datetime
import time
import os


template = """---
topic: {topic}
describe: {describe}
creation date: {date}
type: 案例
tags: []
status: false
链接: {link}
---
"""

def process_github_repo(base_path:str="./test_md",github_path:str = "tqdm/tqdm",overwrite_files:bool=True,
                        filter_directories=None)->None:
    """
    从GitHub存储库读取文档，解析其Markdown内容并创建文件结构.   

    base_path : 基础路径   

    github_path : 目标仓库 tqdm/tqdm   
    """
    owner,repo = github_path.split('/')
    docs_documents = read_github_repo(owner=owner, repo=repo,filter_directories=filter_directories)

    file_dict = {}
    for docs_document in docs_documents:
        file_name = docs_document.metadata.get("file_name")[:-3]
        text = docs_document.text
        structure = parse_markdown_to_custom_structure(markdown_content=text,
                                                       folder_name=file_name)
        file_dict.update(structure)

    create_file_structure(file_dict, base_path=base_path, overwrite=overwrite_files)

def process_github_issues(base_path:str = "./test2_md",github_path:str = "tqdm/tqdm")->None:
    """
    从GitHub读取问题，生成相应的Markdown文件。   

    base_path : 基础路径   
    github_path : 目标仓库 tqdm/tqdm   
    """

    owner,repo = github_path.split('/')
    issue_documents = read_github_issue(owner=owner, repo=repo)

    issues = {}
    for issue_document in issue_documents:
        question = issue_document.text
        http_url = os.path.join(issue_document.metadata.get('url'), 'comments')
        topic = issue_document.metadata.get('url').rsplit('/',1)[1]
        values = template.format(topic = topic,
                        describe ='第 #{topic}问题 ',
                        date = datetime.today().strftime("%Y-%m-%d"),
                        link = http_url)
        issues.update({topic:values + '\n\n' + question})

    for topic,content in issues.items():
        save_to_file(content,base_path,f"{topic}.md")

# answer_markdown = get_comments(http_url)

def get_issue_answer(http_url:str)->str:
    """从url获取问题的回答

    Args:
        http_url (str): 问题的回答的url链接

    Returns:
        str: 回答
    """
    answer_markdown = get_comments(http_url)
    return answer_markdown